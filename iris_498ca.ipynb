{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95803, 2)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "import os\n",
    "cnt = 0\n",
    "user_id,content_type,timestamp,textual=[],[],[],[]\n",
    "with open('./Task2_user_reputation_prediction/Reputation_prediction_data_train.txt') as f:    \n",
    "    for line in f:\n",
    "        data = line.strip().split()\n",
    "        user_id.append(data[-3])\n",
    "        content_type.append(data[-2])\n",
    "        timestamp.append(data[-1])\n",
    "        textual.append(data[:-3])\n",
    "df = DataFrame({'user_id': user_id})\n",
    "# df['user_id']=user_id\n",
    "# df['content_type'] = content_type\n",
    "# df['timestamp'] = timestamp\n",
    "df['textual'] = textual\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('user_id').agg({'textual':'sum'})\n",
    "agg_df=df['textual'].to_frame()\n",
    "agg_df.index=agg_df.index.map(int)\n",
    "agg_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "1       [main, tenet, belief, emerg, church, emerg, wo...\n",
       "2       [christ, death, signific, question, titl, quit...\n",
       "3       [long, jesus, tomb, christian, common, celebr,...\n",
       "5       [pray, peopl, outsid, triniti, understand, rom...\n",
       "6       [christian, bibl, literalist, line, bibl, lite...\n",
       "7       [book, testament, chosen, interest, find, proc...\n",
       "9       [good, news, refer, good, news, differ, jesus,...\n",
       "10      [someon, heaven, faith, sin, forgiven, heaven,...\n",
       "11      [absent, apostol, success, argument, author, c...\n",
       "13      [origin, cathol, apocrypha, cathol, tradit, se...\n",
       "14      [fast, daniel, cyrus, king, persia, reveal, da...\n",
       "15      [differ, judg, god, question, god, answer, man...\n",
       "17      [bibl, stipul, prioriti, person, allegi, fair,...\n",
       "18      [origin, moral, civil, ceremoni, distinct, tes...\n",
       "19      [physic, evid, global, flood, young, earth, cr...\n",
       "21      [biblic, basi, total, deprav, calvin, point, s...\n",
       "22      [role, music, corpor, worship, mani, music, ex...\n",
       "23      [interpret, scriptur, lord, faith, baptism, ep...\n",
       "25      [order, book, protest, bibl, read, bibl, proba...\n",
       "26      [ten, command, usual, refer, exodus, ten, comm...\n",
       "27      [christian, baptis, jesus, make, discipl, nati...\n",
       "29      [symbol, horn, prove, difficult, find, answer,...\n",
       "30      [biblic, literalist, interpret, tale, david, g...\n",
       "31      [biblic, basi, doctrin, triniti, word, term, t...\n",
       "33      [bibl, stand, vegetarian, subject, vegetarian,...\n",
       "34      [signific, jesus, baptism, theolog, baptism, t...\n",
       "35      [martin, luther, main, disagr, cathol, church,...\n",
       "37      [cain, abel, offer, sacrific, law, mose, genes...\n",
       "38      [good, start, point, studi, earli, church, int...\n",
       "39      [origin, christma, tree, accept, christian, tr...\n",
       "                              ...                        \n",
       "3294    [amish, believ, wrong, util, technolog, advanc...\n",
       "3295    [word, term, triniti, bibl, howev, christian, ...\n",
       "3297    [doctrin, individu, guilti, sin, spiritu, fail...\n",
       "3298    [john, remark, stori, jesus, rais, lazarus, de...\n",
       "3299    [short, section, enoch, quot, testament, lette...\n",
       "3301    [chronolog, concept, hell, mention, bibl, ment...\n",
       "3302    [passag, gospel, luke, curious, begin, hear, e...\n",
       "3303    [juda, iscariot, commit, suicid, realiz, evil,...\n",
       "3305    [follow, definit, humanist, andrew, humanist, ...\n",
       "3306    [differ, herod, bibl, herod, great, rule, jesu...\n",
       "3307    [anoth, question, signific, number, answer, re...\n",
       "3309    [bonhoeff, lutheran, pastor, howev, book, cost...\n",
       "3310    [person, told, sinc, discipl, call, jesus, rab...\n",
       "3311    [main, doctrin, disagr, luther, calvin, anybod...\n",
       "3313    [commonplac, christian, today, close, prayer, ...\n",
       "3314    [malachi, send, prophet, elijah, day, lord, ar...\n",
       "3315    [commune, group, jws, brought, app, compar, nw...\n",
       "3317    [heard, mormon, god, actual, fit, descript, su...\n",
       "3318    [respons, question, bibl, explain, exist, foss...\n",
       "3319    [apocalyps, revel, john, book, bibl, difficult...\n",
       "3321    [stack, exchang, believ, core, moder, communit...\n",
       "3322    [theoret, question, interest, system, belief, ...\n",
       "3323    [compet, theori, jesus, hell, damn, jesus, par...\n",
       "3325    [explain, question, jew, consid, mose, abraham...\n",
       "3326    [jesus, brought, everlast, life, abraham, die,...\n",
       "3327    [recent, invit, colleg, student, call, questio...\n",
       "3329    [somewher, travel, read, jesus, consid, adam, ...\n",
       "3330    [triniti, dogma, cathol, church, henc, dogma, ...\n",
       "3331    [naturalist, hypothesi, empti, tomb, resurrect...\n",
       "3333    [peopl, march, feast, patrick, japan, faith, c...\n",
       "Name: textual, Length: 2500, dtype: object"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "5     1\n",
       "6     1"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id, label = [],[]\n",
    "with open('./Task2_user_reputation_prediction/Reputation_prediction_labels_train.txt') as f:    \n",
    "    for line in f:\n",
    "        data = line.strip().split()\n",
    "        user_id.append(data[0])\n",
    "        label.append(data[1])\n",
    "df_test = DataFrame(index=user_id)\n",
    "df_test['label']=label\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df['label']= df_test['label'].tolist()\n",
    "agg_df['textual']=[' '.join(i) for i in agg_df.textual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# a_train, a_test, b_train, b_test = train_test_split(agg_df.textual,agg_df.label , test_size=0.2)\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(a_train)\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB().fit(X, b_train)\n",
    "\n",
    "# Y = vectorizer.transform(a_test)\n",
    "\n",
    "# predicted = clf.predict(Y)\n",
    "# np.mean(predicted == b_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(agg_df.textual)\n",
    "# X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tf_transformer = TfidfTransformer(use_idf=False).fit(X)\n",
    "# X_train_tf = tf_transformer.transform(X)\n",
    "# X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "a_train, a_test, b_train, b_test = train_test_split(X,agg_df.label , test_size=0.1)\n",
    "# train_data,test_data,train_label,test_label\n",
    "# 0.1ï¼Œ6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38400000000000001"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(a_train, b_train)\n",
    "predicted = clf.predict(a_test)\n",
    "np.mean(predicted == b_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_new = ['westminst welfar week', 'worthless']\n",
    "# X_new_counts = vectorizer.transform(docs_new)\n",
    "# X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "\n",
    "# predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "# for doc, category in zip(docs_new, predicted):\n",
    "#      print('%r => %s' % (doc, category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "a_train, a_test, b_train, b_test = train_test_split(agg_df.textual,agg_df.label, test_size=0.1,random_state=10)\n",
    "text_clf.fit(a_train, b_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30399999999999999"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(a_test)\n",
    "np.mean(predicted == b_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52400000000000002"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "#                      ('svm', svm.SVC())\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=17,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n",
    "text_clf.fit(a_train, b_train)  \n",
    "\n",
    "predicted = text_clf.predict(a_test)\n",
    "np.mean(predicted == b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hh/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/hh/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(a_test, predicted,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(a_train, b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha: 0.001\n",
      "tfidf__use_idf: True\n",
      "vect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# gs_clf.predict(['God is love'])[0]\n",
    "gs_clf.best_score_                                  \n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56535319845200882"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
